---
title: "Chandnee & Jada STAT632 Final Project Code"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r}
library(pacman)
pacman::p_load(tidyverse,performance, glmnet, car, MASS)
```

```{r}
# Reading the data in, and loading the data
vino <- read.csv("wine-quality-white-and-red.csv")
glimpse(vino)
```

## To better analyze, we will work only with the white wine data from the set.
```{r}
# remove some variables to clean the data. Since free sulfur.dioxide and total.sulfur.dioxide has same meaning and type is not needed.
vino_w <- vino %>%
  filter(type == "white") %>%
  dplyr::select(-type, -free.sulfur.dioxide) %>%
  drop_na()
```

## We have 4898 rows (Wines) by 11 columns(Aspects).
```{r}
# Cleaned white wine data
dim(vino_w)
view(vino_w)
```

```{r}
#fit multiple linear regression model
lm1 <- lm(quality ~ ., data=vino_w)
summary(lm1)
```
## From summary of full model(lm1), we get 8 predictor(fixed.acidity, volatile acidity, residual sugar, total.sulfur.dioxide, density, pH, sulphates, alcohol) which have most significant effect on the model. We verify the signifigance of these predictors by using AIC.

```{r}
# for variable selection we use AIC
step(lm1)
```
## After using AIC, we can see that we have 8 significant predictors, alike our summary of lm1.

```{r}
# fitting multiple linear regression by removing not significant predictor. Lm1 is the full model, lm2 is the reduced model.
lm2 <- lm(quality ~ fixed.acidity + volatile.acidity 
      + residual.sugar + total.sulfur.dioxide + density + pH + sulphates + alcohol, 
    data = vino_w)

summary(lm2)
```
## After removing the insignifigant predictors, we can see we now have all signifigant predictors in our reduced model (lm2).

```{r}
# partial f test 
anova(lm2, lm1)
```
## From partial f test, we see that full model(lm1) p value is 0.89>.05, so we fail to reject null hypothesis, we can say that we can remove some predictor variables from lm1 full model to get better results.

```{r}
# Scatterplot matrix to anaylze relationships between our predictors and response.
pairs(quality ~ fixed.acidity + volatile.acidity + residual.sugar + 
     + density + pH + sulphates + alcohol, data=vino_w)
```
## Volatile acidity, alcohol, PH and Sulfates show a strong relationship with our response variable quality. Since the data set is considered large, to better analyze the data, we will explore other visualizations. For this project we will focus on our volatile acidity and alcohol predictors.

```{r}
library(ggExtra)
plot_va <- ggplot(data=vino_w, aes(volatile.acidity, quality)) +
geom_point()

ggExtra::ggMarginal(plot_va, type='histogram')

```
## Data appears to be right skewed for variable volatile acidity, log transformation may be useful.

```{r}
plot_al <- ggplot(data=vino_w, aes(alcohol, quality)) +
geom_point()

ggExtra::ggMarginal(plot_al, type='histogram')
```
## Note, our response variable is discrete, between 3 and 9. Thus we will further analyze our data using a regression tree.

```{r}
# Regression tree model to more closely analyze the the data.
library(rpart)
library(rpart.plot)

t1 <- rpart(quality ~ ., data = vino_w)
```

```{r}
# Regression tree to analyze data with a different visualization
par(cex=0.7, xpd=NA)
plot(t1)
text(t1, use.n = TRUE)
```
## From tree model, Alcohol, Volatile acidity, density most impactful predictor in determining wine quality. Now, we will looking at the most impactful variables as defined by our regression tree, with quality as the response.

```{r}
t2 <- rpart(quality ~ alcohol + volatile.acidity, data = vino_w)

par(cex=0.7, xpd=NA)
plot(t2)
text(t2, use.n = TRUE)
```

```{r}
rpart.plot(t2, type = 5)
```
## Here we have a better visualization we can analyze.

## When aclohol content is less than 11 units, and volatile acidity is greater than or equal to 0.25 units, we have a quality rating of 5.4. Where 33% of our data falls within this range. That is, 33% of 4898.

## When aclohol content is greater than 12 units, we have a high quality rating of 6.5. Where 19% of our data falls within this range. That is, 19% of 4898.

## When aclohol is greater than or equal to 11 units, and volatile acidity is less than 0.47 units we have a quality rating of 6.2. Where 18% of our data falls within this range. That is 18% of 4898.

## Thus, approximately 930 observations have a quality rating of 6.5, approximately 881 observations have a quality rating of 6.2, and approximately 1616 observations have a rating of 5.4 with high volatile acidity and lower alcohol content.


```{r}
# Log transformation on volatile acidity, since our marginal histogram from above yielded this transformation may be useful.

lm3<-lm(quality ~ fixed.acidity + log(volatile.acidity) + residual.sugar + total.sulfur.dioxide
     + density + pH + sulphates + alcohol, data=vino_w)
summary(lm3)

# Partial F-test to compare our reduced model lm2 and Transformation model lm3.
anova(lm3, lm2)
```
## In our summary we see all predictors yield signifigance, and the Adj R-squared is 0.2818 while our Adj R-squared for lm2 is 0.2778. We have a 10th of a decimal increase in the value after doing transformation. This shows our transformation is permitted, since we have a slight increase and it is not drastic.

## since p value < .05. Thus, we reject our null hypothesis.

```{r}
# Running AIC to compare models.
AIC(lm1)
AIC(lm2)
AIC(lm3)
```
## We see AIC is lower for our reduced models compared to our full model when using the step function. We will choose lm3 since it has the lowest AIC.

```{r}
# Checking the usual assumptions for a MLR model

par(mfrow=c(1,2), mar=c(2.5, 2.5, 2, 2))
plot(lm3, 1:2)

plot(predict(lm3), rstandard(lm3),xlab="Fitted Values", ylab="Standardized Residuals")
abline(h=0)
qqnorm(rstandard(lm3))
qqline(rstandard(lm3))
hist(resid(lm3))
```
Linearity: The data is nearing linearity, based on standardized Vs. Residuals. Since our response is discrete our model is tilted, but the red line is very close to the dashes line y = 0, thus we can conclude this assumption to be met and lean towards our regression tree for better data analysis.

Independence of Errors: Looking at the residuals vs fitted graph, the correlation should be 0 between y = 0 and the red line on the graph. The red line yields no pattern between the data in the residuals and fitted graph, Thus, our error independence assumption appears met.

Normality of Errors: The residuals must be approximately normally distributed. Examining the QQ plot, we see there are still values that stray from our normal line, there are presence of outliers. Looking at out histogram, it appears slightly left skwewed. And thus we may have a violation. Since this is more so a real life example, we can consider this histogram plot to be sufficient for our purposes.

Equal Variances: This is shown by the residuals vs fitted graph. The variance of residuals are the same across all values on the x-axis, and there appears to be a pattern, but this is due to our discrete response variable. so we can consider this assumption met.

```{r}
# Plot leverage points
plot(hatvalues(lm3), rstandard(lm3),
xlab='Leverage', ylab='Standardized Residuals')
p<- 8
n<- nrow(vino_w)
abline(v = 2*(p+1)/n, lty=2)
abline(h = c(-4,4), lty=2)
```

```{r}
# Taking out values above 4.
ind <- which(hatvalues(lm3) > 0.01 & abs(rstandard(lm3)) > 4)
vino_w[ind, ]
```
## Identifying these points to investigate. The two observations differ vastly in quality rating, total sulfur dioxide content, and residual sugar. While both have a higher content of alcohol, wine2782 has a quality rating of 6, total sulfur dioxide content of 160, residual suagr of 65.8 While wine4746 has a quality rating of 3, total sulfur dioxide 440, residual sugar 2.9. The vast difference in quality rating could be influenced by the amount of sugar wine4746 has as opposed to wine2782. These observations being outliers could be due to the fact that the residual sugar content is so high and so low.

```{r}
# Remove outlier from above graph
vino_clean <- vino_w[-ind,]
```

```{r}
# make plot after removing outlier
lm4 <- lm(quality ~ fixed.acidity + log(volatile.acidity) + residual.sugar + total.sulfur.dioxide +
       density + pH + sulphates + alcohol, data=vino_clean)
summary(lm4)
AIC(lm4)
```
## After removing the outliers and making pplot, we now have all significant preditor variables. We also have an Adj R-squared of 0.2861 which is 100th of a decimal more than our Adj R-squared from lm2 0.2818. Thus we have a super slight improvement. We may not need to make further adjustments to our model since this increase is so small.

## From lm4 we will provide an interpretation of the signs of the estimated coefficients.

## log(volatile.acidity): has a negative value. So for every 1 unit log increase in volatile acidity, the quality rating of wine will decrease.

## alcohol: has a positive value. So, for every 1 unit increase in alcohol content, the quality rating of wine will increase. 

## This analysis is in alignment with our regression tree visualization. Needless to say, if I want to consume a better quality wine, I should look for higher alcohol content.

```{r}
results<- matrix(c(.2775 ,11131.04,.2778 ,11127.28,.2818 ,11100.03, .2861, 11057.74 ) ,ncol=4)
rownames(results) <- c("Adjusted R^2", "AIC")
colnames(results) <-c("Full(lm1)", "Reduced(lm2)", "Trans (lm3)","Removing outlier(lm4)")
results <- as.table(results)
results
```